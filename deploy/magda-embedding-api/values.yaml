global:
  image: {}
  rollingUpdate: {}

# -- (bool) Start Fastify app in debug mode with nodejs inspector
# inspector port is 9320
debug: false

# -- (string) The log level of the application.
# one of 'fatal', 'error', 'warn', 'info', 'debug', 'trace'; 
# also 'silent' is supported to disable logging. 
# Any other value defines a custom level and requires supplying a level value via levelVal.
logLevel: "warn"

# -- (int) The maximum amount of time in milliseconds in which a fastify plugin can load. 
# If not, ready will complete with an Error with code 'ERR_AVVIO_PLUGIN_TIMEOUT'.
# @default -- Default to 180000 (180 seconds).
pluginTimeout: 180000

# -- (int) Defines the maximum payload, in bytes, that the server is allowed to accept
# @default -- Default to 10485760 (10MB).
bodyLimit: 10485760

# -- (int) The maximum amount of time before forcefully closing pending requests.
# This should set to a value lower than the Pod's termination grace period (which is default to 30s)
# @default -- Default to 25000 (25s).
closeGraceDelay: 25000

# -- (object) Application configuration of the service.
# You can supply a list of key-value pairs to be used as the application configuration.
# Currently, the only supported config field is `modelList`.
# Via the `modelList` field, you can specify a list of LLM models that the service supports.
# Although you can specify multiple models, only one model will be used at this moment.
# Each model item have the following fields: 
# <ul>
# <li> `name` (string): The huggingface registered model name. We only support ONNX model at this moment. This field is required. </li>
# <li> `default` (bool): Optional; Whether this model is the default model. If not specified, the first model in the list will be the default model. Only default model will be loaded. </li>
# <li> `quantized` (bool): Optional; Whether the quantized version of model will be used. If not specified, the quantized version model will be loaded. </li>
# <li> `config` (object): Optional; The configuration object that will be passed to the model.  </li>
# <li> `cache_dir` (string): Optional; The cache directory of the downloaded models. If not specified, the default cache directory will be used. </li>
# <li> `local_files_only` (bool): Optional; Whether to only load the model from local files. If not specified, the model will be downloaded from the huggingface model hub. </li>
# <li> `revision` (string) Optional, Default to 'main'; The specific model version to use. It can be a branch name, a tag name, or a commit id. 
#   Since we use a git-based system for storing models and other artifacts on huggingface.co, so `revision` can be any identifier allowed by git. NOTE: This setting is ignored for local requests. </li>
# <li> `model_file_name` (string) Optional; </li>
# <li> `extraction_config` (object) Optional; The configuration object that will be passed to the model extraction function for embedding generation. <br/>
#   <ul>
#   <li> `pooling`: ('none' or 'mean' or 'cls') Default to 'none'. The pooling method to use. </li>
#   <li> `normalize`: (bool) Default to true. Whether or not to normalize the embeddings in the last dimension. </li>
#   <li> `quantize`: (bool) Default to `false`. Whether or not to quantize the embeddings. </li>
#   <li> `precision`: ("binary" or "ubinary") default to "binary". The precision to use for quantization. Only used when `quantize` is true. </li>
#   </ul> 
#  </li>
# </ul>
# Please note: The released docker image only contains "Alibaba-NLP/gte-base-en-v1.5" model. 
# If you specify other models, the server will download the model from the huggingface model hub at the startup.
# You might want to adjust the `startupProbe` settings to accommodate the model downloading time.
# Depends on the model size, you might also want to adjust the `resources.limits.memory` & `resources.requests.memory`value.
appConfig: {}

# image setting loadding order: (from higher priority to lower priority)
# - Values.image.x
# - Values.defaultImage.x
# You can set `imagePullSecret` value to `false` (bool) to reset the value (see example below)

image: 
  name: "magda-embedding-api"
  # repository: 
  # pullPolicy: 
  # imagePullSecret: 

defaultImage:
  repository: ghcr.io/magda-io
  pullPolicy: IfNotPresent
  imagePullSecret: false

nameOverride: ""
fullnameOverride: ""

replicas: 1

autoscaling:
  hpa:
    enabled: false
    minReplicas: 1
    maxReplicas: 3
    targetCPU: 90
    targetMemory: ""  

priorityClassName: magda-9

podSecurityContext:
  runAsNonRoot: true
  runAsUser: 1000

rbac:
  create: false
  serviceAccountAnnotations: {}
  serviceAccountName: ""
  # -- Controls whether or not the Service Account token is automatically mounted to /var/run/secrets/kubernetes.io/serviceaccount
  automountServiceAccountToken: false

hostAliases: []

startupProbe:
  httpGet:
    path: /status/startup
    port: 3000
  periodSeconds: 10
  timeoutSeconds: 30
  failureThreshold: 30
  successThreshold: 1
  initialDelaySeconds: 10

livenessProbe:
  httpGet:
    path: /status/liveness
    port: 3000
  periodSeconds: 20
  timeoutSeconds: 30
  failureThreshold: 10
  successThreshold: 1
  initialDelaySeconds: 10

readinessProbe:
  httpGet:
    path: /status/readiness
    port: 3000
  periodSeconds: 20
  timeoutSeconds: 5
  failureThreshold: 10
  successThreshold: 1
  initialDelaySeconds: 10

extraEnvs: []
#  - name: "NODE_OPTIONS"
#    value: "--max-old-space-size=1800"

envFrom: []

extraVolumes: []
  # - name: extras
  #   emptyDir: {}

extraVolumeMounts: []
  # - name: extras
  #   mountPath: /usr/share/extras
  #   readOnly: true

extraInitContainers: ""

extraContainers: ""

service:
  name: "magda-embedding-api"
  type: ClusterIP
  # The IP family and IP families options are to set the behaviour in a dual-stack environment
  # Omitting these values will let the service fall back to whatever the CNI dictates the defaults
  # should be
  #
  # ipFamilyPolicy: SingleStack
  # ipFamilies:
  # - IPv4
  port: 80
  targetPort: 3000
  loadBalancerIP: ""
  nodePort: ""
  labels: {}
  annotations: {}
  loadBalancerSourceRanges: []
  # 0.0.0.0/0
  httpPortName: http

nodeSelector: {}

tolerations: []

affinity: {}

# -- This is the pod topology spread constraints
# https://kubernetes.io/docs/concepts/workloads/pods/pod-topology-spread-constraints/
topologySpreadConstraints: []

# -- pod lifecycle policies as outlined here:
# https://kubernetes.io/docs/concepts/containers/container-lifecycle-hooks/#container-hooks
lifecycle: {}

deploymentAnnotations: {}
podAnnotations: {}

resources:
  requests:
    cpu: "100m"
    # -- (string) the memory request of the container
    # Once the model is loaded, the memory usage of the service for serving request would be much lower. Set to 850M for default model.
    memory: "850M"
  limits:
    # -- (string) the memory limit of the container
    # Due to [this issue of ONNX runtime](https://github.com/microsoft/onnxruntime/issues/15080), the peak memory usage of the service is much higher than the model file size. 
    # When change the default model, be sure to test the peak memory usage of the service before setting the memory limit.
    # non-quantized model will be used by default, the memory limit is set to 2000M to accommodate the default non-quantized perk memory usage during the initial loading.
    # If you use a quantized model, you can set the memory limit to a lower value (1100M peak memory would be sufficient).
    memory: "2000M"